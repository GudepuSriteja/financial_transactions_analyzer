# Financial Transactions Analyzer

A clean, beginner-friendly yet professional Python project that ingests raw transaction data, cleans it safely, validates it against financial rules, and produces meaningful analytical outputs.

This project is designed to demonstrate **real-world data engineering practices**: correctness over cleverness, auditability, and clean Git workflow.

---

##  What This Project Does

The pipeline processes raw transaction data and produces:

* *Valid transactions* (clean, rule-compliant)
* *Rejected transactions* (with clear rejection reasons)
* *User-level spend analysis*

It explicitly avoids silent data corruption and makes every decision traceable.

---

## ðŸ§  Why This Project Exists

Many beginner projects:

* silently drop bad data
* mix cleaning and validation
* rely on brittle row-by-row logic
* look correct but fail in edge cases

This project intentionally avoids those traps and follows practices used in real analytics / data teams.

---

##  Project Structure

```
financial-transactions-analyzer/
â”‚
â”œâ”€â”€ analyzer/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ loader.py        # Reads raw CSV data
â”‚   â”œâ”€â”€ cleaner.py       # Safe data normalization (no corruption)
â”‚   â”œâ”€â”€ validator.py     # Business rule enforcement & auditability
â”‚   â”œâ”€â”€ analysis.py      # Aggregations and analytics
â”‚   â””â”€â”€ anomalies.py     # (reserved for future anomaly detection)
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/
â”‚       â””â”€â”€ transactions.csv   # Sample input data
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_cleaner_*.py       # Unit tests for cleaning logic
â”‚
â”œâ”€â”€ main.py                     # Pipeline orchestrator
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

##  Data Flow (End-to-End)

```
Raw CSV
   â†“
Loader (read file)
   â†“
Cleaner (normalize safely)
   â†“
Validator (business rules)
   â†“
Valid CSV  +  Rejected CSV (with reasons)
   â†“
Analysis outputs
```

Each step has **one responsibility** and does it deterministically.

---

##  Validation Rules Implemented

Transactions are **rejected** if they violate any of the following:

* Missing required fields
* Invalid or zero/negative amount
* Amount exceeds a sanity upper bound
* Unsupported currency
* Future timestamp
* Duplicate transaction IDs

Rejected rows are **not dropped** â€” they are saved with explicit reasons.

---

##  How to Run the Project

###  Setup

```bash
python -m venv venv
source venv/bin/activate   # Windows: venv\Scripts\activate
pip install -r requirements.txt
```

###  Run the pipeline

```bash
python main.py
```

###  Outputs

Generated files (ignored by Git):

* `data/processed/valid_transactions.csv`
* `data/processed/rejected_transactions.csv`
* `data/processed/user_spend_summary.csv`

---

##  Tests

Unit tests validate that:

* text fields are normalized correctly
* empty and malformed values are handled safely

Run tests:

```bash
pytest
```

---

## ðŸ›  Design Principles Followed

* **No silent data loss**
* **Vectorized pandas operations** (no iterrows)
* **Timezone-safe timestamps**
* **Auditability over convenience**
* **Beginner-readable, reviewer-friendly code**

---

## ðŸ”® Future Improvements

* Add anomaly detection (outliers, fraud signals)
* Add logging instead of print statements
* Add CLI arguments (input/output paths)
* Add integration tests
* Add visualization layer

---


##  Final Note

This is a small project, but itâ€™s built with the same habits used in real data teams: be explicit, avoid surprises, and keep the pipeline easy to understand.
